% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/model-ebm.R
\name{ebm}
\alias{ebm}
\alias{gcm}
\alias{ebm_j}
\title{Exemplar-based Cognitive Models}
\usage{
gcm(
  formula,
  class,
  data,
  fix = NULL,
  options = NULL,
  similarity = "minkowski",
  ...
)

ebm_j(
  formula,
  criterion,
  data,
  fix = NULL,
  options = NULL,
  similarity = "minkowski",
  ...
)

ebm(formula, criterion, data, mode, fix = NULL, options = NULL, ...)
}
\arguments{
\item{formula}{A \link[stats:formula]{formula}, the variables in \code{data} to be modeled. For example, \code{y ~ x1 + x2 | x3 + x4} models response y as function of two stimuli with features x1, x2 and x3, x4 (respectively). Lines \code{|} separate stimuli.}

\item{class}{A \link[stats:formula]{formula}, the variable in \code{data} with the feedback about the true class/category. For example \code{~ cat}. \code{NA}s are interpreted as trials without feedback (partial feedback, see details).}

\item{data}{A data frame, the data to be modeled.}

\item{fix}{(optional) A list or the string \code{"start"}, the fixed model parameters, if missing all parameters are estimated. Model parameter names depend on \code{formula}, \code{class} and can be \emph{\code{x1}, \code{x2}, \code{lambda}, \code{r}, \code{q}, \code{b0}, \code{b1}} (see details - model parameters).
\itemize{
\item \code{list(r = 2.70)} sets parameter \emph{\code{r}} equal to 2.70.
\item \code{list(r = "q")} sets parameter \emph{\code{r}} equal to parameter \emph{\code{q}} (estimates \emph{\code{q}}).
\item \code{list(q = "r", r = 2.70)} sets parameter \emph{\code{q}} equal to parameter \emph{\code{r}} and sets \emph{\code{r}} equal to 2.70 (estimates none of the two).
\item \code{list(r = NA)} omits the parameter \emph{\code{r}}, if possible.
\item \code{"start"} sets all parameters equal to their initial values (estimates none). Useful for building a first test model.
}}

\item{options}{(optional) A list, list entries change the modeling procedure. For example, \code{list(lb = c(k=0))} changes the lower bound of parameter \emph{k} to 0, or \code{list(fit_measure = "mse")} changes the goodness of fit measure in parameter estimation to mean-squared error,  for all options, see \code{\link[=cm_options]{cm_options()}}.}

\item{similarity}{(optional) A string, similarity function, currently only \code{"minkowski"}.}

\item{criterion}{A \link[stats:formula]{formula}, the variable in \code{data} with the feedback about the continous criterion. For example, \code{~ val} \code{NA}s are interpreted as trials without feedback (partial feedback, see details).}

\item{mode}{(optional) A string, the response mode, can be \code{"discrete"} or \code{"continuous"}, can be abbreviated. If missing, will be inferred from \code{criterion}.}

\item{choicerule}{A string, the choice rule. Allowed values, see  \code{cm_choicerules()}: \code{"none"} is no choice rule, \code{"softmax"} is \href{https://en.wikipedia.org/wiki/Softmax_function}{soft-maximum}, \code{"luce"} is \href{https://en.wikipedia.org/wiki/Luce\%27s_choice_axiom}{Luce's axiom}.}
}
\value{
Returns a cognitive model object, which is an object of class \href{Cm}{cm}. A model, that has been assigned to \code{m}, can be summarized with \code{summary(m)} or \code{anova(m)}. The parameter space can be viewed using \verb{pa. rspace(m)}, constraints can be viewed using \code{constraints(m)}.
}
\description{
\code{ebm()} fits an exemplar-based model.
\itemize{
\item \code{gcm()} fits a generalized context model (aka. exemplar model) for discrete responses (Medin & Schaffer, 1978; Nosofsky, 1986)
\item \code{ebm_j()} fits an exemplar-based judgment model for continuous responses (Juslin et al., 2003)
}
}
\details{
The model can predict new data - \code{predict(m, newdata = ...)} - and this is how it works:
\itemize{
\item If \code{newdata}s \code{criterion} or \code{class} variable has only \code{NA}s, the model predicts using the originally supplied \code{data} as exemplar-memory. Parameters are not re-fit.
\item If \code{newdata}'s' \code{criterion} or \code{class} variable has values other than \code{NA}, the model predicts the first row in \code{newdata} using the originally-supplied \code{data} as exemplars in memory, but predictions for subsequent rows of \code{newdata} use also the criterion values in new data. In other words, exemplar memory is \emph{extended} by exemplars in new data for which a criterion exists. Parameters are not re-fit.
}
\subsection{Model Parameters}{

The model has the following free parameters, depending on the model specification (see \code{\link[=npar]{npar()}}). A model with formula \code{~ x1 + x2} has parameters:
\itemize{
\item \emph{\strong{\verb{x1, x2}}} (dynamic names) are attention weights, their names correspond to the right side of \code{formula}.
\item \emph{\strong{\code{lambda}}} is the sensitivity, larger values make the similarity decrease more steeply with higher distance metric.
\item \emph{\strong{\code{r}}} is the order of the Minkowski distance metric (2 is an Euclidean metric, 1 is a city-block metric).
\item \emph{\strong{\code{q}}} is the shape of the relation between similarity and distance, usually equal to \emph{\code{r}}.
\item In \code{gcm()}:
\itemize{
\item \emph{\strong{\verb{b0, b1}}} (dynamic names) is the bias towards categories, their names are \code{b} plus the unique values of \code{class}. For example \code{b0} is the bias for class = 0.
\item If \code{choicerule = "softmax"}: \emph{\strong{\code{tau}}}  is the temperature or choice softness, higher values cause more equiprobable choices. If \code{choicerule = "epsilon"}: \emph{\strong{\code{eps}}} is the error proportion, higher values cause more errors from maximizing.
}
}
}

\subsection{Partial Feedback}{

Regarding \code{NA} values in \code{class} or \code{criterion}: The model takes \code{NA} values in the class/criterion variable as trials without feedback, in which a stimulus was shown but no feedback about the class or criterion was given (partial feedback paradigm). The model predicts the class or criterion for such trials without feedback based on the previous exemplar(s) for which feedback was shown. The model ignores the trials without feedback in the prediction of the subsequent trials.
}
}
\examples{
# Make some fake data
D <- data.frame(f1 = c(0,0,1,1,2,2,0,1,2),     # feature 1
                f2 = c(0,1,2,0,1,2,0,1,2),     # feature 2
                cl = c(0,1,0,0,1,0,NA,NA,NA),  # criterion/class
                 y = c(0,0,0,1,1,1,0,1,1))     # participant's responses

M <- gcm(y ~ f1+f2, class= ~cl, D, fix="start",
         choicerule = "none")                  # GCM, par. fixed to start val.

predict(M)                                     # predict 'pred_f', pr(cl=1 | features, trial)
M$predict()                                    # -- (same) --
summary(M)                                     # summary
anova(M)                                       # anova-like table
logLik(M)                                      # Log likelihood
M$logLik()                                     # -- (same) --
M$MSE()                                        # mean-squared error
M$npar()                                       # 7 parameters
M$get_par()                                    # parameter values
M$coef()                                       # 0 free parameters


### Specify models
# -------------------------------
gcm(y ~ f1 + f2, class = ~cl, D, 
    choicerule = "none")                          # GCM (has bias parameter)
ebm(y~f1+f2, criterion=~cl, D, mode="discrete",
    choicerule = "none")                          # -- (same) --
ebm_j(y ~ f1 + f2, criterion = ~cl, D)              # Judgment EBM  (no bias par.)
ebm(y~f1+f2, criterion=~cl, D, mode="continuous")   # -- (same) --


### Specify parameter estimation
# -------------------------------
gcm(y~f1+f2, ~cl, D, fix=list(b0=0.5, b1=0.5),
     choicerule = "none")                       # fix 'bias' par. to 0.5, fit 5 par
gcm(y~f1+f2, ~cl, D, fix=list(f1=0.9,f2=0.1),
     choicerule = "none")                       # fix attention 'f1' to 90 \%  f1 & fit 5 par
gcm(y~f1+f2, ~cl, D, fix=list(q=2, r=2),
     choicerule = "none")                      # fix 'q', 'q' to 2 & fit 5 par
gcm(y~f1+f2, ~cl, D, fix=list(q=1, r=1),
     choicerule = "none")                      # fix 'q', 'r' to 1 & fit 5 par
gcm(y~f1+f2, ~cl, D, fix=list(lambda=2),
     choicerule = "none")                      # fix 'lambda' to 2 & fit 6 par
gcm(y~f1+f2, ~cl, D, fix="start", 
    choicerule = "none")                        # fix all par to start val. 
}
\references{
{Medin, D. L., & Schaffer, M. M. (1978). Context theory of classification learning. \emph{Psychological Review, 85}, 207-238. \url{http://dx.doi.org/10.1037//0033-295X.85.3.207}}

{Nosofsky, R. M. (1986). Attention, similarity, and the identification-categorization relationship. \emph{Journal of Experimental Psychology: General, 115}, 39-57. \url{http://dx.doi.org/10.1037/0096-3445.115.1.39}}

{Juslin, P., Olsson, H., & Olsson, A.-C. (2003). Exemplar effects in categorization and multiple-cue judgment. \emph{Journal of Experimental Psychology: General, 132}, 133-156. \url{http://dx.doi.org/10.1037/0096-3445.132.1.133}}
}
\seealso{
Other cognitive models: 
\code{\link{baseline_const_c}()},
\code{\link{bayes}()},
\code{\link{choicerules}},
\code{\link{cpt}},
\code{\link{hm1988}()},
\code{\link{shortfall}},
\code{\link{utility}}
}
\author{
Jana B. Jarecki, \email{jj@janajarecki.com}
}
\concept{cognitive models}
